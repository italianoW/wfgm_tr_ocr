{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f091663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel, TrOCRProcessor\n",
    "import torch\n",
    "\n",
    "# Load the model\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8235135b",
   "metadata": {},
   "source": [
    "1. Quantas camadas tem o modelo TrOCR?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20363987",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(model.encoder.encoder.layer):\n",
    "    print(f\"Bloco {i} ->\", type(layer))\n",
    "for i, layer in enumerate(model.decoder.model.decoder.layers):\n",
    "    print(f\"Bloco {i} ->\", type(layer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b40e2f9",
   "metadata": {},
   "source": [
    "2. Quais os módulos de todas as camadas do TrOCR?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4b451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.encoder.named_modules():\n",
    "    print(name, \"->\", type(module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f749b199",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.decoder.named_modules():\n",
    "    print(name, \"->\", type(module))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2337dca5",
   "metadata": {},
   "source": [
    "As principais camadas do encoder(ViT) são as camadas de Embedding, Self-Attention, Feed-Forward, Layer Norm e Residual Connection\n",
    "As principais camadas do decoder(Texto) são as camadas de Embedding, Self-Attention, Cross-Attention, Feed-Forward e Layer Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d0e969",
   "metadata": {},
   "source": [
    "3. Quantos paramêtros tem o TrORC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacd54a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_params = sum(p.numel() for p in model.encoder.parameters())\n",
    "print(\"Parâmetros do Encoder:\", encoder_params)\n",
    "\n",
    "decoder_params = sum(p.numel() for p in model.decoder.parameters())\n",
    "print(\"Parâmetros do Decoder:\", decoder_params)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Parâmetros Totais:\", total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfccba40",
   "metadata": {},
   "source": [
    "4. Quais os paramêtros de cada camada?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e902648",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.encoder.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b90a21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.decoder.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afa76fa",
   "metadata": {},
   "source": [
    "RESUMO GERAL:\n",
    "\n",
    "Self-Attention é um mecanismo que permite que cada token preste atenção nos outros tokens da mesma sequência para entender melhor o contexto.\n",
    "\n",
    "q(Query) = \"O que eu quero buscar?\"\n",
    "k(Key) = \"O que eu ofereço?\"\n",
    "v(Value) = \"O que eu passo adiante?\"\n",
    "\n",
    "Cross-Attention é o mecanismo que permite que o decoder olhe para a saída do encoder e use essa informação para gerar o próximo token corretamente.\n",
    "\n",
    "O Feed-Forward processa cada token de forma independente, transformando o vetor de atenção em algo mais rico e não-linear.\n",
    "\n",
    "LayerNorm mantém os vetores dos tokens estáveis e equilibrados em todas as subcamadas do Transformer.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
